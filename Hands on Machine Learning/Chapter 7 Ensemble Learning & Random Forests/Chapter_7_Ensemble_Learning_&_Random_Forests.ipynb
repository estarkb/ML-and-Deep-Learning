{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 7 - Ensemble Learning & Random Forests",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "ntwGl86m3I67",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Chapter 7 - [Ensemble Learning & Random Forests](https://github.com/ageron/handson-ml2/blob/master/07_ensemble_learning_and_random_forests.ipynb)**\n",
        "\n",
        "Este algoritmo se asimila a lo que llamamos \"la sabiduría de las masas\", es decir, que la predicción agregada de varios predictores (SVM, Regrsiones, Arboles de Decisión, etc) pueden entregar un mejor resultado que la predicción de un experto. \n",
        "\n",
        "Este método hace lo mismo, une a varios predictores. Por tanto, será usual entrenar una serie de muy buenos modelos, luego agregarlos a través de un método de ensemble y así obtener un resultado todavía mejor. Hacer click [aquí](https://www.netflixprize.com/) para más curiosidades sobre esta metodología de Machine Learning."
      ]
    },
    {
      "metadata": {
        "id": "Apx60W1NLvSb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Importamos Librerías Típicas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "57mZ8kgFvyCf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***Datos***"
      ]
    },
    {
      "metadata": {
        "id": "tMIWZTuSvxXK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fEFKgmGJ0gbj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### **7.1 VotingClassifiers**"
      ]
    },
    {
      "metadata": {
        "id": "O0SfU7Hov4_G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####***a. Estimadores***\n",
        "\n",
        "Aquí utilizamos todo el poder múltiples clasificadores. Con ellos podremos unir lo bueno y separar lo malo de cada uno. Esto ocurre porque cada Algoritmo analiza patrones de una manera específica, al ser así, cada uno está sujeto a perder información que otros sí consideran. \n",
        "\n",
        "Por tanto, realizar la estimación de manera unida provoca que dichas debilidades se \"mitiguen\" siempre y cuando los errores sean independientes. Esto último es imposible siempre que entrenemos al algoritmo con los mismo datos. Sin embargo, podemos salvarnos de esto utilizando algoritmos MUY distintos.\n",
        "\n",
        "El funcionamiento en términos más duros es a través de ver las probabilidades que cada clasificador le asigna a cada observación. En base a dichos valores el VotingClassifier analiza cuales son las clasificaciones más seguras.\n",
        "\n",
        "cambiandoles desde \"hard\" a \"soft\" muchas veces obtendremos mejores resultados, ya que lo hecho en el último parráfo tendrá mayor relevancia.\n"
      ]
    },
    {
      "metadata": {
        "id": "aaqznN-yuPoq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "log_clf = LogisticRegression(solver=\"lbfgs\")\n",
        "rnd_clf = RandomForestClassifier(n_estimators=100)\n",
        "svm_clf = SVC(probability=True, gamma=\"scale\")\n",
        "\n",
        "voting_clf = VotingClassifier(estimators=[(\"lr\", log_clf), (\"rf\", rnd_clf), (\"svc\", svm_clf)],\n",
        "                             voting=\"soft\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DkAuFOG2z0T6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####***b. Entrenamos el modelo***"
      ]
    },
    {
      "metadata": {
        "id": "cjOrULi0wUGa",
        "colab_type": "code",
        "outputId": "6627a509-a074-4866-e382-70701a1a2aac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "cell_type": "code",
      "source": [
        "voting_clf.fit(X_train, y_train)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
              "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
              "          tol=0.0001, verbose=0, warm_start=False)), ('rf', RandomF...',\n",
              "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
              "  tol=0.001, verbose=False))],\n",
              "         flatten_transform=None, n_jobs=None, voting='soft', weights=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "aI3E7yDNz3Rp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####***c. Vemos que tan bueno es***\n",
        "\n",
        "Aquí nos interesa saber como le va a cada clasificador por separado, así podemos ver el verdadero poder de ***VotingClassifier***\n",
        "\n",
        "Los resultados hablan solos, esta vez se ocupó \"soft\" en la metodología de votos. Esto porque obtiene mejores resultados."
      ]
    },
    {
      "metadata": {
        "id": "M5kohY1zwW5U",
        "colab_type": "code",
        "outputId": "2400210c-a72a-47f3-f2b8-c94964ea1649",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
        "  clf.fit(X_train, y_train)\n",
        "  y_pred = clf.predict(X_test)\n",
        "  print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression 0.864\n",
            "RandomForestClassifier 0.896\n",
            "SVC 0.896\n",
            "VotingClassifier 0.912\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UCqyeKUy0xHr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### **7.2 Bagging & Pasting**\n",
        "\n",
        "***Bagging***: Consiste en tomar muestras aleatorias a partir de la muestra de entrenamiento. Y con ellas realizar las estimaciones. Así se espera obtener un resultado más cercano al valor verdadero\n",
        "***Pasting***: Lo mismo, pero cambia una cosa. Las observaciones que se sacan de la muestra para cada submuestra no se vuelve a repetir.\n",
        "\n",
        "El resultado final de esta metodología es uno agregado de cada predicción en la submuestra. Este tendrá un sesgo similar, pero con menor varianza que si entrenasemos un unico predictor sobre la muestra.\n",
        "\n",
        "Estos métodos son muy utiles para muestras grandes."
      ]
    },
    {
      "metadata": {
        "id": "bANhyIN42FHY",
        "colab_type": "code",
        "outputId": "8d049500-e6b4-49a7-a4a1-ed12ca0412b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n",
        "                           max_samples=100, bootstrap=True, n_jobs=1)\n",
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred = bag_clf.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.912"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "hucHVnY13XFm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Lo bueno de esta metodología es que generaliza mucho mejor que realizar un arbol de decisión solo. Esto porque las predicciones son hechas sobre datos más diversos, ya que cada sub muestra añade nueva información que de lo contrario sería desapercibida."
      ]
    },
    {
      "metadata": {
        "id": "0TXICPrdAXtZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### **a. Evaluación Out-of-Bag **\n",
        "\n",
        "Con Bagging algunas de las observaciones es probable que no sea usadas para crear submuestras. Esto no lo queremos, ya que no queremos perder información.\n",
        "\n",
        "Pero no todo es malo, ya que estas observaciones nunca antes vistas pueden servir de validación para el modelo."
      ]
    },
    {
      "metadata": {
        "id": "wfXc8hjcBEmh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "931fbfc6-4ab6-44ce-e808-1c2196956bd0"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n",
        "                           max_samples=100, bootstrap=True, n_jobs=1, oob_score=True)\n",
        "bag_clf.fit(X_train, y_train)\n",
        "bag_clf.oob_score_"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9253333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "729qLeEYBazv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### **7.3 Random Forests**\n",
        "\n",
        "Es un Arbol de Decisión hecho con metodología Bagging, aportando mayor diversidad a la estimación, ya que no se busca el estimador con mejor fit, sino que el mejor estimador entre muestras aleatorias. Lo último suma mayor sesgo, pero menor varianza. Entregando como resultado mejores estimaciones usualmente.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "2qPltm2zBkN4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "8c079d8b-58cb-434a-82d8-1dd2916e72a7"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=1)\n",
        "rnd_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rnd_clf.predict(X_test)\n",
        "y_pred_rf"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
              "       0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
              "       0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
              "       1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
              "       1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "vnbOsKXaC03a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Adelante, tenemos un algoritmo que basicamente hace lo mismo que el anterior. Un Arbol de Decisión con Bagging como metodología. "
      ]
    },
    {
      "metadata": {
        "id": "qkkRegEAB73e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bag_clf = BaggingClassifier(DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\n",
        "                           n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gj1qItouIuwF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "1adb437b-2dd7-4215-8440-5a4f5e0e25b3"
      },
      "cell_type": "code",
      "source": [
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred_bag = bag_clf.predict(X_test)\n",
        "y_pred_bag"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
              "       1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
              "       0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
              "       1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
              "       1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "0b3hXSmyDLUY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###**7.4 Feature Importance**\n",
        "\n",
        "Un método de RandomForest que le aporta gran interpretabilidad al algoritmo. A través de dar a conocer cuanta impureza resta cada atributo al modelo general."
      ]
    },
    {
      "metadata": {
        "id": "Z9eKs55tDxjR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "461c32e3-1707-4598-86f6-f303535e0d60"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=1)\n",
        "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
        "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
        "  print(name, score)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sepal length (cm) 0.09852921942033192\n",
            "sepal width (cm) 0.02283996462832791\n",
            "petal length (cm) 0.4300195278980297\n",
            "petal width (cm) 0.4486112880533106\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DDp7Fc5pEe6m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###**7.5 Boosting**\n",
        "\n",
        "La idea general es hacer que el computador calcule múltiples estimadores, cada uno de estos intentando corregir al anterior, o en simple, superando al anterior. Los dos más conocidos son **AdaBoost** y **GradientBoosting**"
      ]
    },
    {
      "metadata": {
        "id": "VCbbkqiYFmzk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####**a. AdaBoost**"
      ]
    },
    {
      "metadata": {
        "id": "HPGUqHk2EeZX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "5da36887-7479-4e92-dc66-f72b8aa5eec4"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
        "                            algorithm=\"SAMME.R\", learning_rate=0.5)\n",
        "ada_clf.fit(X_train, y_train)\n",
        "#Si hace Overfit reduzco el número de estimadores"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AdaBoostClassifier(algorithm='SAMME.R',\n",
              "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
              "            max_features=None, max_leaf_nodes=None,\n",
              "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "            min_samples_leaf=1, min_samples_split=2,\n",
              "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
              "            splitter='best'),\n",
              "          learning_rate=0.5, n_estimators=200, random_state=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "PtVpNp0OFYwv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "36cdf6b1-5570-4b4e-d0d8-9fdc9ce11f64"
      },
      "cell_type": "code",
      "source": [
        "y_pred = ada_clf.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.896"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "Irghn-zkFlxN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####**b. GradienBoosting**"
      ]
    },
    {
      "metadata": {
        "id": "Qg1L2mZ4F3Qs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "fecb2f64-9aff-42bd-b3d5-f9b97b943853"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
        "gbrt.fit(X_train, y_train)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
              "             learning_rate=1.0, loss='ls', max_depth=2, max_features=None,\n",
              "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
              "             min_impurity_split=None, min_samples_leaf=1,\n",
              "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
              "             n_estimators=3, n_iter_no_change=None, presort='auto',\n",
              "             random_state=None, subsample=1.0, tol=0.0001,\n",
              "             validation_fraction=0.1, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "8Vyz_8SJG52Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "68c5b89e-1482-44b4-f981-9269683b458f"
      },
      "cell_type": "code",
      "source": [
        "gbrt_slow = GradientBoostingRegressor(max_depth=2, n_estimators=200, learning_rate=0.1, random_state=42)\n",
        "gbrt_slow.fit(X, y)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
              "             learning_rate=0.1, loss='ls', max_depth=2, max_features=None,\n",
              "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
              "             min_impurity_split=None, min_samples_leaf=1,\n",
              "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
              "             n_estimators=200, n_iter_no_change=None, presort='auto',\n",
              "             random_state=42, subsample=1.0, tol=0.0001,\n",
              "             validation_fraction=0.1, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "Yd8xJjJ5HQwK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3f38bb7b-254d-488e-ccf0-8c57d3e709dc"
      },
      "cell_type": "code",
      "source": [
        "#número de estimadores óptimos\n",
        "gbrt.n_estimators"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "LCvQNedyHqFs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**A continuación elementos no presentes en mi edición del libro (1era), los copié directo**"
      ]
    },
    {
      "metadata": {
        "id": "iQGs-MvcH050",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**[XGBoost](https://xgboost.readthedocs.io/en/latest/python/python_api.html)**\n",
        "\n",
        "Iteraciones recursivas hasta alcanzar el menor error posible en la muestra de testeo"
      ]
    },
    {
      "metadata": {
        "id": "hyoWfHL8HlvS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "try:\n",
        "    import xgboost\n",
        "except ImportError as ex:\n",
        "    print(\"Error: the xgboost library is not installed.\")\n",
        "    xgboost = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vuaaQoRkHnd7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5319b2d0-f8ef-407b-9b79-c3b4650a21e6"
      },
      "cell_type": "code",
      "source": [
        "if xgboost is not None:  # not shown in the book\n",
        "    gbrt = xgboost.XGBRegressor(random_state=42)\n",
        "    gbrt.fit(X_train, y_train)\n",
        "    y_pred = gbrt.predict(X_test)\n",
        "    val_error = mean_squared_error(y_test, y_pred) # Not shown\n",
        "    print(\"Validation MSE:\", val_error)           # Not shown"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation MSE: 0.07582886953097483\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TWRUG-YcHpUp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        },
        "outputId": "fa55ec34-bde5-428d-faa2-5ce8a906fe15"
      },
      "cell_type": "code",
      "source": [
        "if xgboost is not None:  # not shown in the book\n",
        "    xgb_reg.fit(X_train, y_train,\n",
        "                eval_set=[(X_test, y_test)], early_stopping_rounds=2)\n",
        "    y_pred = xgb_reg.predict(X_test)\n",
        "    val_error = mean_squared_error(y_test, y_pred)  # Not shown\n",
        "    print(\"Validation MSE:\", val_error)            # Not shown"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]\tvalidation_0-rmse:0.467243\n",
            "Will train until validation_0-rmse hasn't improved in 2 rounds.\n",
            "[1]\tvalidation_0-rmse:0.438661\n",
            "[2]\tvalidation_0-rmse:0.414096\n",
            "[3]\tvalidation_0-rmse:0.392648\n",
            "[4]\tvalidation_0-rmse:0.375086\n",
            "[5]\tvalidation_0-rmse:0.359023\n",
            "[6]\tvalidation_0-rmse:0.346346\n",
            "[7]\tvalidation_0-rmse:0.334524\n",
            "[8]\tvalidation_0-rmse:0.324476\n",
            "[9]\tvalidation_0-rmse:0.316143\n",
            "[10]\tvalidation_0-rmse:0.309492\n",
            "[11]\tvalidation_0-rmse:0.30367\n",
            "[12]\tvalidation_0-rmse:0.299046\n",
            "[13]\tvalidation_0-rmse:0.294084\n",
            "[14]\tvalidation_0-rmse:0.290521\n",
            "[15]\tvalidation_0-rmse:0.287067\n",
            "[16]\tvalidation_0-rmse:0.283539\n",
            "[17]\tvalidation_0-rmse:0.27956\n",
            "[18]\tvalidation_0-rmse:0.276707\n",
            "[19]\tvalidation_0-rmse:0.272834\n",
            "[20]\tvalidation_0-rmse:0.27208\n",
            "[21]\tvalidation_0-rmse:0.269625\n",
            "[22]\tvalidation_0-rmse:0.268827\n",
            "[23]\tvalidation_0-rmse:0.267\n",
            "[24]\tvalidation_0-rmse:0.26684\n",
            "[25]\tvalidation_0-rmse:0.264461\n",
            "[26]\tvalidation_0-rmse:0.264167\n",
            "[27]\tvalidation_0-rmse:0.262823\n",
            "[28]\tvalidation_0-rmse:0.262675\n",
            "[29]\tvalidation_0-rmse:0.262055\n",
            "[30]\tvalidation_0-rmse:0.261691\n",
            "[31]\tvalidation_0-rmse:0.261615\n",
            "[32]\tvalidation_0-rmse:0.261679\n",
            "[33]\tvalidation_0-rmse:0.261317\n",
            "[34]\tvalidation_0-rmse:0.261483\n",
            "[35]\tvalidation_0-rmse:0.261628\n",
            "Stopping. Best iteration:\n",
            "[33]\tvalidation_0-rmse:0.261317\n",
            "\n",
            "Validation MSE: 0.06844920656054865\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}